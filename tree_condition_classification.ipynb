{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e316146f-c5ca-470b-9405-1d72e17240b6",
   "metadata": {},
   "source": [
    "### Paper parameters \n",
    "- 60% train, 40% split into val and test\n",
    "- Images randomly cropped to 300x300 pixels\n",
    "- 4 classes for classification\n",
    "- cross entropy loss\n",
    "- 15 epochs\n",
    "- 0.001 learning rate (start, then annealing cosine scheduler)\n",
    "- Sliding window of 300x300 with a stride of 50 pixels\n",
    "    - Softmax per window. Highest class probability is one vote if probability higher than threshold.\n",
    "    - Final prediction is the condition with highest number of votes across all windows\n",
    "    - Probability thresholds of 0.7. No probability, then data is dropped \n",
    "\n",
    "### Paper results\n",
    "- field/ no field classifier: Finally, training a CNN to classify\n",
    "field/not field improved precision to 0.98 and lowered recall\n",
    "slightly to 0.95 — a worthwhile trade-of\n",
    "- \"Maize had the lowest F1 score under both WebCC\n",
    "and iNaturalist (62% and 68%, respectively), followed by\n",
    "sugarcane (62% and 76%). The low performance is likely\n",
    "due to the more visual similarities between the two crops at\n",
    "the early-growth stage as well as their similar height. Meanwhile, rice has short and bright green leaves and cassava has\n",
    "a palmate leaf structure and is grown more separately, both\n",
    "more visually distinctive from the street\".\n",
    "    - And we discriminate from subtle differences in leaf color and amount for very different tree species.\n",
    "- \"The top performing model trained on the Expert labeled dataset achieved 82% accuracy when directly classifying the whole image, 90% with sliding windows but no\n",
    "MHP threshold, and 93% with a 0.90 MHP threshold\"\n",
    "\n",
    "### Useful references\n",
    "+ Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "+ Croptypes paper: https://arxiv.org/pdf/2309.05930.pdf\n",
    "+ Input needed for resnet50: https://pytorch.org/hub/pytorch_vision_resnet/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6d6df3-0d58-4feb-b783-65cae25d4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2 as tranforms_v2\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Additional Setup to use Tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42345aaf-a146-451f-822d-ccc225652a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_class_distribution(dataset):\n",
    "    count_dict = dict()\n",
    "    \n",
    "    for inputs, label in dataset:\n",
    "        count_dict[label] = count_dict.get(label, 0) + 1\n",
    "    return count_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dea98-68d5-4b2b-a48d-bb952c365eee",
   "metadata": {},
   "source": [
    "Use ordered_trees_classified.zip to get te correct folder order to work with the code. The images will be cropped to allow for 9 windows. Horizontal stride 50 pixels and vertical stride 38 pixels. The smaller stride is due to small image height which either needs very large or somewhat small stride. \n",
    "\n",
    "Due to the cropped size, the image can be cropped again randomly during training of the model whilst still having the window within a region where the testing windows will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a77052-330f-4e03-b440-a20dba2a0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((300, 324)),  # Cut out the image to 300x324 | To be further cut up later\n",
    "    transforms.ToTensor(),           # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "data_path = \"trees_classified\"\n",
    "label_transform = {0:\"Matig\", 1:\"Redelijk\", 2:\"Slecht\", 3:\"Goed\", 4:\"Dood\", 5:\"Zeer-Slecht\"}  # Dataset sorted on most samples first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e7964d-e456-4824-84d4-53a818350ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a7145e-436c-4ff8-96d4-acf6a68f1c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 9553, 1: 8104, 2: 947, 3: 289, 4: 116, 5: 87}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how unbalanced the classes are before balancing\n",
    "get_class_distribution(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5749a7a5-81f8-43aa-9a06-39741857e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the classes by randomly samling class_limit samples from each class\n",
    "\n",
    "class_limit = 1000\n",
    "num_classes = 3  # specify how many classes we use. This determines output dimension of the model.\n",
    "\n",
    "#idx_class1 = [i for i, label in enumerate(dataset.targets) if label == 1]  # Add one if samples are weighted based on class imbalance\n",
    "idx_class0 = [i for i, label in enumerate(dataset.targets) if label == 0]\n",
    "idx_class1 = [i for i, label in enumerate(dataset.targets) if label == 1]\n",
    "idx_class2 = [i for i, label in enumerate(dataset.targets) if label == 2]\n",
    "\n",
    "\n",
    "np.random.shuffle(idx_class0)\n",
    "np.random.shuffle(idx_class1)\n",
    "idx_class0_limit = idx_class0[:class_limit]\n",
    "idx_class1_limit = idx_class1[:class_limit]\n",
    "# print(len(idx_class1_limit))\n",
    "# print(len(idx_class0))\n",
    "\n",
    "idx_dataset_limited = np.concatenate((idx_class0_limit, idx_class1_limit, idx_class2))\n",
    "# print(len(idx_dataset_limited))\n",
    "\n",
    "balanced_dataset = Subset(dataset, idx_dataset_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f209f7fa-ae3d-4f8b-a25f-3f1c580f56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(balanced_dataset))\n",
    "test_size = len(balanced_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(balanced_dataset, [train_size, test_size])\n",
    "\n",
    "# Make some into validation to evaluate the model before to find out which weight perform best during training. \n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb803df-f9d3-4735-8cc5-a92ec166a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall dataset balance {0: 1000, 1: 1000, 2: 947}\n",
      "Train dataset balance {0: 727, 2: 675, 1: 719}\n",
      "Test dataset balance {1: 195, 0: 205, 2: 190}\n",
      "Val dataset balance {0: 68, 1: 86, 2: 82}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate class balance per set\n",
    "print(f\"Overall dataset balance {get_class_distribution(balanced_dataset)}\")\n",
    "print(f\"Train dataset balance {get_class_distribution(train_dataset)}\")\n",
    "print(f\"Test dataset balance {get_class_distribution(test_dataset)}\")\n",
    "print(f\"Val dataset balance {get_class_distribution(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996866e2-340b-442b-8c76-3ba35908f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c941c69e-50c5-468f-87a8-dca6ac6716ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch all at once = torch.Size([32, 3, 224, 224])\n",
      "One image before and after\n",
      "Original size = torch.Size([3, 300, 324])\n",
      "transformed size = torch.Size([3, 220, 220])\n",
      "\n",
      "The input tensor has a single mode: 2\n"
     ]
    }
   ],
   "source": [
    "# Testing transforms and sizes\n",
    "random_crop = transforms.RandomCrop((224, 224))\n",
    "\n",
    "for data in train_loader:\n",
    "    samples, labels = data\n",
    "    print(\"Minibatch all at once =\", random_crop(samples).size())\n",
    "    #sample_list = []\n",
    "    for sample in samples:\n",
    "        #sample_list.append(sample)\n",
    "        print(\"One image before and after\")\n",
    "        print(\"Original size =\", sample.size())\n",
    "        sample_trans = tranforms_v2.functional.crop(sample, 0,138,220, 220)\n",
    "        print(\"transformed size =\",sample_trans.size())\n",
    "        break\n",
    "    #sample_list = 32 * [torch.Tensor(1)]\n",
    "    #stacked_tensor = torch.cat(sample_list)\n",
    "    #print(\"Stacked size =\", stacked_tensor.size())\n",
    "    break\n",
    "\n",
    "# Mode did not work correctly for multiple mode values. ChatGPT helped me find a workaround. \n",
    "unique_values, counts = torch.unique(torch.tensor([1, 0, 0, 2, 2, 2]), return_counts=True)\n",
    "max_freq = torch.max(counts)\n",
    "modes = unique_values[counts == max_freq]\n",
    "if len(modes) > 1:\n",
    "    print(\"\\nThe input tensor has multiple modes:\", modes)\n",
    "else:\n",
    "    print(\"\\nThe input tensor has a single mode:\", modes.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "622c3a3a-d1d5-4598-9a45-bc9c8908bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/niek/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████████████████████████████████| 97.8M/97.8M [00:05<00:00, 17.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(weights='DEFAULT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa5e780f-dc7c-48c3-9aae-1b17a8590de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_crop = transforms.RandomCrop((224, 224))\n",
    "\n",
    "def train(train_loader, net, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Trains network for one epoch in batches.\n",
    "\n",
    "    Args:\n",
    "        train_loader: Data loader for training set.\n",
    "        net: Neural network model.\n",
    "        optimizer: Optimizer (e.g. SGD).\n",
    "        criterion: Loss function (e.g. cross-entropy loss).\n",
    "    \"\"\"\n",
    "\n",
    "    avg_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # iterate through batches\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = random_crop(data[0]), data[1]  # Crop images randomly for testing\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero teh parameters of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fwd + back + opti\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of loss and acc\n",
    "        avg_loss += loss\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return avg_loss/len(train_loader), 100 * correct/total\n",
    "\n",
    "def test(test_loader, net, criterion, device):\n",
    "    avg_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    labels_pred = []\n",
    "    labels_true = []\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Apply sliding window on every image of the batch\n",
    "            for image, label in zip(inputs, labels):\n",
    "                image_windows = []\n",
    "                for t in [0, 38, 76]:  # top pixels coordinate\n",
    "                    for l in [138, 188, 238]:  # left pixels coordinate\n",
    "                         image_windows.append(tranforms_v2.functional.crop(image, t, l, 224, 224))\n",
    "                window_tensor = torch.stack(image_windows)\n",
    "                labels_tensor = torch.cat(9 * [torch.tensor([label])])\n",
    "                \n",
    "                window_tensor, labels_tensor = window_tensor.to(device), labels_tensor.to(device)\n",
    "\n",
    "                outputs = net(window_tensor)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Calculate modes and their occurance (torch.modes only gives lowest mode)\n",
    "                unique_values, counts = torch.unique(predicted, return_counts=True)\n",
    "                max_freq = torch.max(counts)\n",
    "                modes = unique_values[counts == max_freq]\n",
    "                \n",
    "                # Check if there is one mode, only then there is a majority vote and valid result\n",
    "                if len(modes) == 1:\n",
    "                    aggregated_label = modes.item()\n",
    "                    \n",
    "                    # Average continuous loss over the windows and later over images\n",
    "                    loss = criterion(outputs, labels_tensor)\n",
    "                    avg_loss += loss  # now avg_loss is per image, not per batch. Devide by total instead of batch number in the end. \n",
    "                \n",
    "                    total += 1\n",
    "                    correct += int(aggregated_label == label)\n",
    "                    labels_pred.append(aggregated_label)\n",
    "                    labels_true.append(label)\n",
    "    \n",
    "    return avg_loss/total, 100 * correct/total, labels_pred, labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c84f90e-bbc8-4fb2-9dbc-eb7bca1a587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3e7d7d9-fa2b-4dbb-94d3-b5b4a9f80dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████▍                         | 36/100 [59:14<1:45:18, 98.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=\"./runs_condition/\", flush_secs=300)\n",
    "\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.fc.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "resnet50.to(device)\n",
    "\n",
    "# Set the number of epochs to for training\n",
    "epochs = 100\n",
    "\n",
    "patience = 20\n",
    "\n",
    "val_acc_best = 0\n",
    "patience_cnt = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "    # Train on data\n",
    "    train_loss, train_acc = train(train_loader, resnet50, optimizer, criterion, device)\n",
    "\n",
    "    # Test on data\n",
    "    val_loss, val_acc,_,_ = test(val_loader, resnet50, criterion, device)\n",
    "\n",
    "    # Write metrics to Tensorboard\n",
    "    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Test':val_loss}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': train_acc,'Test':val_acc} , epoch)\n",
    "\n",
    "\n",
    "    if val_acc > val_acc_best:\n",
    "      val_acc_best = val_acc\n",
    "      patience_cnt = 0\n",
    "      best_model_wts = resnet50.state_dict()\n",
    "\n",
    "    else:\n",
    "      patience_cnt += 1\n",
    "      if patience_cnt == patience:\n",
    "        break\n",
    "    # print(f\"Current loss {train_loss} at epoch {epoch}\")\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de215daf-2307-4c04-96f3-7c88ee5885b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-534edd8bdee492a6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-534edd8bdee492a6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open Tensorboard\n",
    "%tensorboard --logdir runs_condition/balanced/\n",
    "\n",
    "# For local users only: uncomment the last line, run this cell once and wait for\n",
    "# it to time out, run this cell a second time and you should see the board.\n",
    "# %tensorboard --logdir runs/ --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fcade1d-256f-4d88-8179-ad97fa3f1338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73ad9d96-66c1-41b6-b203-2530873f96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet50.state_dict(), \"resnet50_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec6a15-5d26-4b14-8509-fdd6fcfe76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if the training stage cell is not loaded into memory\n",
    "num_classes = 6\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "resnet50.load_state_dict(torch.load(\"resnet50_model_imbalance.pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.fc.parameters(), lr=0.001)\n",
    "resnet50.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b195e1c-c9e1-4009-8742-f1ee29891046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report per class\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        64\n",
      "           1       0.41      0.86      0.55        83\n",
      "           2       0.60      0.37      0.45        82\n",
      "\n",
      "    accuracy                           0.44       229\n",
      "   macro avg       0.34      0.41      0.33       229\n",
      "weighted avg       0.36      0.44      0.36       229\n",
      "\n",
      "Confusion matrix\n",
      "[[ 0 54 10]\n",
      " [ 2 71 10]\n",
      " [ 2 50 30]]\n",
      "\n",
      "Micro (global F1\n",
      "0.4410480349344978\n",
      "\n",
      "Accuracy: 44.10480349344978, Average loss: 1.0646237134933472\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "resnet50.eval()\n",
    "\n",
    "\n",
    "# Iterate through the test data\n",
    "test_loss, test_acc, predicted_labels, true_labels = test(val_loader, resnet50, criterion, device)\n",
    "\n",
    "print(f\"Class labels: {label_transform}\")  # Class 0 is Matig, 1 is Redelijk, 2 is Slecht\n",
    "\n",
    "# Print the evaluation methods\n",
    "print(f\"Report per class\\n{classification_report(true_labels, predicted_labels)}\")\n",
    "\n",
    "print(f\"Confusion matrix\\n{confusion_matrix(true_labels, predicted_labels)}\")\n",
    "\n",
    "#print(f\"\\nPer class F1 score\\n{f1_score(true_labels, predicted_labels, average=None)}\")  # Per class F1\n",
    "\n",
    "print(f\"\\nWeighted F1: {f1_score(true_labels, predicted_labels, average='weighted')}, Micro (global) F1: {f1_score(true_labels, predicted_labels, average='micro')}\")  # Get global F1, not per class average\n",
    "\n",
    "print(f\"\\nAccuracy: {test_acc}, Balanced accuray: {balanced_accuracy_score(true_labels, predicted_labels)}\\nAverage loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4e43c-673a-486e-8103-16c1ba979aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
