{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e316146f-c5ca-470b-9405-1d72e17240b6",
   "metadata": {},
   "source": [
    "### Paper parameters \n",
    "- 60% train, 40% split into val and test\n",
    "- Images randomly cropped to 300x300 pixels\n",
    "- 4 classes for classification\n",
    "- cross entropy loss\n",
    "- 15 epochs\n",
    "- 0.001 learning rate (start, then annealing cosine scheduler)\n",
    "- Sliding window of 300x300 with a stride of 50 pixels\n",
    "    - Softmax per window. Highest class probability is one vote if probability higher than threshold.\n",
    "    - Final prediction is the condition with highest number of votes across all windows\n",
    "    - Probability thresholds of 0.7. No probability, then data is dropped \n",
    "\n",
    "### Paper results\n",
    "- field/ no field classifier: Finally, training a CNN to classify\n",
    "field/not field improved precision to 0.98 and lowered recall\n",
    "slightly to 0.95 â€” a worthwhile trade-of\n",
    "- \"Maize had the lowest F1 score under both WebCC\n",
    "and iNaturalist (62% and 68%, respectively), followed by\n",
    "sugarcane (62% and 76%). The low performance is likely\n",
    "due to the more visual similarities between the two crops at\n",
    "the early-growth stage as well as their similar height. Meanwhile, rice has short and bright green leaves and cassava has\n",
    "a palmate leaf structure and is grown more separately, both\n",
    "more visually distinctive from the street\".\n",
    "    - And we discriminate from subtle differences in leaf color and amount for very different tree species.\n",
    "- \"The top performing model trained on the Expert labeled dataset achieved 82% accuracy when directly classifying the whole image, 90% with sliding windows but no\n",
    "MHP threshold, and 93% with a 0.90 MHP threshold\"\n",
    "\n",
    "### Useful references\n",
    "+ Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "+ Croptypes paper: https://arxiv.org/pdf/2309.05930.pdf\n",
    "+ Input needed for resnet50: https://pytorch.org/hub/pytorch_vision_resnet/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b6d6df3-0d58-4feb-b783-65cae25d4dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2 as tranforms_v2\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Additional Setup to use Tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42345aaf-a146-451f-822d-ccc225652a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_class_distribution(dataset):\n",
    "    count_dict = dict()\n",
    "    \n",
    "    for input, label in dataset:\n",
    "        count_dict[label] = count_dict.get(label, 0) + 1\n",
    "    return count_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dea98-68d5-4b2b-a48d-bb952c365eee",
   "metadata": {},
   "source": [
    "Use ordered_trees_classified.zip to get te correct folder order to work with the code. The images will be cropped to allow for 9 windows. Horizontal stride 50 pixels and vertical stride 36 pixels. The smaller stride is due to small image height which either needs very large or somewhat small stride. \n",
    "\n",
    "Due to the cropped size, the image can be cropped againrandomly during training of the model whilst still having the window in a place where some windows will be during testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73a77052-330f-4e03-b440-a20dba2a0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((300, 324)),  # Cut out the image to 300x324 | To be further cut up later\n",
    "    transforms.ToTensor(),           # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "data_path = \"trees_classified\"\n",
    "label_transform = {0:\"Matig\", 1:\"Redelijk\", 2:\"Slecht\", 3:\"Goed\", 4:\"Dood\", 5:\"Zeer-Slecht\"}  # Dataset sorted on most samples first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8e7964d-e456-4824-84d4-53a818350ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14a7145e-436c-4ff8-96d4-acf6a68f1c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 9553, 1: 8104, 2: 947, 3: 289, 4: 116, 5: 87}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how unbalanced the classes are before balancing\n",
    "get_class_distribution(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5749a7a5-81f8-43aa-9a06-39741857e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1000, 1: 1000, 2: 947}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the classes by randomly samling class_limit samples from each class\n",
    "\n",
    "class_limit = 1000\n",
    "num_classes = 3  # specify how many classes we use. This determines output dimension of the model.\n",
    "\n",
    "#idx_class1 = [i for i, label in enumerate(dataset.targets) if label == 1]  # Add one if samples are weighted based on class imbalance\n",
    "idx_class0 = [i for i, label in enumerate(dataset.targets) if label == 0]\n",
    "idx_class1 = [i for i, label in enumerate(dataset.targets) if label == 1]\n",
    "idx_class2 = [i for i, label in enumerate(dataset.targets) if label == 2]\n",
    "\n",
    "\n",
    "np.random.shuffle(idx_class0)\n",
    "np.random.shuffle(idx_class1)\n",
    "idx_class0_limit = idx_class0[:class_limit]\n",
    "idx_class1_limit = idx_class1[:class_limit]\n",
    "# print(len(idx_class1_limit))\n",
    "# print(len(idx_class0))\n",
    "\n",
    "idx_dataset_limited = np.concatenate((idx_class0_limit, idx_class1_limit, idx_class2))\n",
    "# print(len(idx_dataset_limited))\n",
    "\n",
    "balanced_dataset = Subset(dataset, idx_dataset_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f209f7fa-ae3d-4f8b-a25f-3f1c580f56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(balanced_dataset))\n",
    "test_size = len(balanced_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(balanced_dataset, [train_size, test_size])\n",
    "\n",
    "# Make some into validation to evaluate the model before to find out which weight perform best during training. \n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcb803df-f9d3-4735-8cc5-a92ec166a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall dataset balance {0: 1000, 1: 1000, 2: 947}\n",
      "Train dataset balance {1: 730, 2: 665, 0: 726}\n",
      "Test dataset balance {2: 211, 0: 189, 1: 190}\n",
      "Val dataset balance {2: 71, 1: 80, 0: 85}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate class balance per set\n",
    "print(f\"Overall dataset balance {get_class_distribution(balanced_dataset)}\")\n",
    "print(f\"Train dataset balance {get_class_distribution(train_dataset)}\")\n",
    "print(f\"Test dataset balance {get_class_distribution(test_dataset)}\")\n",
    "print(f\"Val dataset balance {get_class_distribution(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "996866e2-340b-442b-8c76-3ba35908f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c941c69e-50c5-468f-87a8-dca6ac6716ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch all at once = torch.Size([32, 3, 224, 224])\n",
      "One image before and after\n",
      "Original size = torch.Size([3, 300, 324])\n",
      "transformed size = torch.Size([3, 220, 220])\n",
      "\n",
      "The input tensor has a single mode: 2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Testing transforms and sizes\n",
    "random_crop = transforms.RandomCrop((224, 224))\n",
    "\n",
    "for data in train_loader:\n",
    "    samples, labels = data\n",
    "    print(\"Minibatch all at once =\", random_crop(samples).size())\n",
    "    #sample_list = []\n",
    "    for sample in samples:\n",
    "        #sample_list.append(sample)\n",
    "        print(\"One image before and after\")\n",
    "        print(\"Original size =\", sample.size())\n",
    "        sample_trans = tranforms_v2.functional.crop(sample, 0,138,220, 220)\n",
    "        print(\"transformed size =\",sample_trans.size())\n",
    "        break\n",
    "    #sample_list = 32 * [torch.Tensor(1)]\n",
    "    #stacked_tensor = torch.cat(sample_list)\n",
    "    #print(\"Stacked size =\", stacked_tensor.size())\n",
    "    break\n",
    "\n",
    "# Mode did not work correctly for multiple mode values. ChatGPT helped me find a workaround. \n",
    "unique_values, counts = torch.unique(torch.tensor([1, 0, 0, 2, 2, 2]), return_counts=True)\n",
    "max_freq = torch.max(counts)\n",
    "modes = unique_values[counts == max_freq]\n",
    "if len(modes) > 1:\n",
    "    print(\"\\nThe input tensor has multiple modes:\", modes)\n",
    "else:\n",
    "    print(\"\\nThe input tensor has a single mode:\", modes.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "622c3a3a-d1d5-4598-9a45-bc9c8908bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(weights='DEFAULT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa5e780f-dc7c-48c3-9aae-1b17a8590de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_crop = transforms.RandomCrop((224, 224))\n",
    "\n",
    "def train(train_loader, net, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Trains network for one epoch in batches.\n",
    "\n",
    "    Args:\n",
    "        train_loader: Data loader for training set.\n",
    "        net: Neural network model.\n",
    "        optimizer: Optimizer (e.g. SGD).\n",
    "        criterion: Loss function (e.g. cross-entropy loss).\n",
    "    \"\"\"\n",
    "\n",
    "    avg_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # iterate through batches\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = random_crop(data[0]), data[1]  # Crop images randomly for testing\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero teh parameters of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fwd + back + opti\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of loss and acc\n",
    "        avg_loss += loss\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return avg_loss/len(train_loader), 100 * correct/total\n",
    "\n",
    "def test(test_loader, net, criterion, device):\n",
    "    avg_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    labels_pred = []\n",
    "    labels_true = []\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Apply sliding window on every image of the batch\n",
    "            for image, label in zip(inputs, labels):\n",
    "                image_windows = []\n",
    "                for t in [0, 36, 72]:  # top pixels coordinate\n",
    "                    for l in [138, 188, 238]:  # left pixels coordinate\n",
    "                         image_windows.append(tranforms_v2.functional.crop(image, t, l, 224, 224))\n",
    "                window_tensor = torch.stack(image_windows)\n",
    "                labels_tensor = torch.cat(9 * [torch.Tensor(label)])\n",
    "                \n",
    "                window_tensor, labels_tensor = window_tensor.to(device), labels_tensor.to(device)\n",
    "\n",
    "                outputs = net(window_tensor)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Calculate modes and their occurance (torch.modes only gives lowest mode)\n",
    "                unique_values, counts = torch.unique(predicted, return_counts=True)\n",
    "                max_freq = torch.max(counts)\n",
    "                modes = unique_values[counts == max_freq]\n",
    "                \n",
    "                # Check if there is one mode, only then there is a majority vote and valid result\n",
    "                if len(modes) = 1:\n",
    "                    aggregated_label = modes.item()\n",
    "                    \n",
    "                    # Average continuous loss over the windows and later over images\n",
    "                    loss = criterion(outputs, labels_tensor)\n",
    "                    avg_loss += loss  # now avg_loss is per image, not per batch. Devide by total instead of batch number in the end. \n",
    "                \n",
    "                    total += 1\n",
    "                    correct += int(aggregated_label == label)\n",
    "                    labels_pred.append(aggregated_label)\n",
    "                    labels_true.append(label)\n",
    "    \n",
    "    return avg_loss/total, 100 * correct/total, labels_pred, labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c84f90e-bbc8-4fb2-9dbc-eb7bca1a587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3e7d7d9-fa2b-4dbb-94d3-b5b4a9f80dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/15 [02:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m patience_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Train on data\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Test on data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#val_loss, val_acc = test(val_loader, resnet50, criterion, device)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Write metrics to Tensorboard\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalars(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m:val_loss}, epoch)\n",
      "Cell \u001b[1;32mIn[36], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# keep track of loss and acc\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Niek\\Documents\\TU Delft\\Master Robotics\\Deep Learning\\croptypes\\venv\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Niek\\Documents\\TU Delft\\Master Robotics\\Deep Learning\\croptypes\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=\"./runs_condition/\", flush_secs=300)\n",
    "\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.fc.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "resnet50.to(device)\n",
    "\n",
    "# Set the number of epochs to for training\n",
    "epochs = 100\n",
    "\n",
    "patience = 15\n",
    "\n",
    "train_acc_best = 0\n",
    "patience_cnt = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "    # Train on data\n",
    "    train_loss, train_acc = train(train_loader, resnet50, optimizer, criterion, device)\n",
    "\n",
    "    # Test on data\n",
    "    val_loss, val_acc,_,_ = test(val_loader, resnet50, criterion, device)\n",
    "\n",
    "    # Write metrics to Tensorboard\n",
    "    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Test':val_loss}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': train_acc,'Test':val_acc} , epoch)\n",
    "\n",
    "\n",
    "    if val_acc > val_acc_best:\n",
    "      val_acc_best = val_acc\n",
    "      patience_cnt = 0\n",
    "      best_model_wts = resnet50.state_dict()\n",
    "\n",
    "    else:\n",
    "      patience_cnt += 1\n",
    "      if patience_cnt == patience:\n",
    "        break\n",
    "    # print(f\"Current loss {train_loss} at epoch {epoch}\")\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de215daf-2307-4c04-96f3-7c88ee5885b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Tensorboard\n",
    "%tensorboard --logdir runs_condition/\n",
    "\n",
    "# For local users only: uncomment the last line, run this cell once and wait for\n",
    "# it to time out, run this cell a second time and you should see the board.\n",
    "# %tensorboard --logdir runs/ --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcade1d-256f-4d88-8179-ad97fa3f1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad9d96-66c1-41b6-b203-2530873f96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet50.state_dict(), \"resnet50_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b195e1c-c9e1-4009-8742-f1ee29891046",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "resnet50.eval()\n",
    "\n",
    "\n",
    "# Iterate through the test data\n",
    "test_loss, test_acc, predicted_labels, true_labels = test(val_loader, resnet50, criterion, device)\n",
    "\n",
    "# Print the evaluation methods\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "\n",
    "print(f1_score(true_labels, predicted_labels))  # Per class F1\n",
    "\n",
    "print(f1_score(true_labels, predicted_labels, average='micro'))  # Get global F1, not per class average\n",
    "\n",
    "print(f\"Accuracy: {test_acc}, Average loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac50997-d971-472b-bf9a-4d7d8ad00195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d05a4f-9590-4e41-b75e-e47b7ebc0a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6e5bd-9ae7-4f10-ae6a-5003f3f5c36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4e43c-673a-486e-8103-16c1ba979aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
